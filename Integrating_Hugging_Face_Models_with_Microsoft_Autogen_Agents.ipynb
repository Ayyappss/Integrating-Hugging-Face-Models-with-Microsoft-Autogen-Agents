{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "KorBzTfc8hq6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your Bearer Token\n",
        "bearer_token = \"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\"\n"
      ],
      "metadata": {
        "id": "mZB3URQZ-TCs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2 tokenizer with authentication token\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", revision=\"main\", use_auth_token=bearer_token)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF7OL2tJ-jCc",
        "outputId": "c39b4a59-4832-4311-9c8f-9785513ace65"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:769: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "[INFO|configuration_utils.py:733] 2024-06-12 15:03:49,712 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-12 15:03:49,720 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-12 15:03:49,723 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-12 15:03:49,727 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-12 15:03:49,732 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-12 15:03:49,736 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-12 15:03:49,737 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2108] 2024-06-12 15:03:49,739 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:733] 2024-06-12 15:03:49,740 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-12 15:03:49,743 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GPT-2 model\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", revision=\"main\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TktoxV_-k5R",
        "outputId": "9a33eab2-2d41-4b64-c59b-812d2d5ee060"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[INFO|configuration_utils.py:733] 2024-06-12 15:04:09,438 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json\n",
            "[INFO|configuration_utils.py:796] 2024-06-12 15:04:09,446 >> Model config GPT2Config {\n",
            "  \"_name_or_path\": \"gpt2\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"reorder_and_upcast_attn\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"scale_attn_by_inverse_layer_idx\": false,\n",
            "  \"scale_attn_weights\": true,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"transformers_version\": \"4.41.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:3474] 2024-06-12 15:04:09,455 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors\n",
            "[INFO|configuration_utils.py:962] 2024-06-12 15:04:09,497 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:4280] 2024-06-12 15:04:11,128 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
            "\n",
            "[INFO|modeling_utils.py:4288] 2024-06-12 15:04:11,133 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
            "[INFO|configuration_utils.py:917] 2024-06-12 15:04:11,218 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json\n",
            "[INFO|configuration_utils.py:962] 2024-06-12 15:04:11,220 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"eos_token_id\": 50256\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for the text generation agent using Hugging Face model\n",
        "class TextGenAgent:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def generate(self, input_text):\n",
        "        inputs = self.tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "        outputs = self.model.generate(inputs)\n",
        "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "Ez5bOH3W-nv2"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for user proxy agent\n",
        "class UserProxyAgent:\n",
        "    pass"
      ],
      "metadata": {
        "id": "x4Ada6HS-tHt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for assistant agent\n",
        "class AssistantAgent:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "q1vAWRwb-xLz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for QA agent\n",
        "class QAAgent:\n",
        "    def answer(self, input_text):\n",
        "        return f\"Answering question: {input_text}\""
      ],
      "metadata": {
        "id": "j93DRwxb-ysj"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Placeholder for code generation agent\n",
        "class CodeAgent:\n",
        "    def generate(self, input_text):\n",
        "        return f\"Generating code for: {input_text}\"\n"
      ],
      "metadata": {
        "id": "c4GMtwAh-0Oy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Integration with local LLM using TextGenAgent\n",
        "text_gen_agent = TextGenAgent(model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "COjrfIHG-1y6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up Userproxy agent & Assistant agent\n",
        "user_proxy_agent = UserProxyAgent()\n",
        "assistant_agent = AssistantAgent()"
      ],
      "metadata": {
        "id": "MT1oxGpu-3jG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom function handler to map external function calls to agent chat\n",
        "def custom_function_handler(input_text):\n",
        "    # Process the input_text and return the appropriate response\n",
        "    return f\"Custom response for: {input_text}\"\n"
      ],
      "metadata": {
        "id": "L68LfHLC-5QV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up QA agent & code generation agent\n",
        "qa_agent = QAAgent()\n",
        "code_agent = CodeAgent()\n"
      ],
      "metadata": {
        "id": "DujO_8gT-7OB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple interaction example\n",
        "def run_demo():\n",
        "    # Simulate a user query\n",
        "    user_queries = [\n",
        "        \"How do I implement a function in Python?\",\n",
        "        \"What is the capital of France?\",\n",
        "        \"Explain the concept of neural networks.\",\n",
        "        \"What are the benefits of using Python for data science?\",\n",
        "        \"Generate a Python function to calculate the factorial of a number.\",\n",
        "    ]\n",
        "\n",
        "    for query in user_queries:\n",
        "        # Use the text generation agent to generate a response\n",
        "        response = text_gen_agent.generate(query)\n",
        "        print(f\"Text Generation Response for '{query}': {response}\\n\")\n",
        "\n",
        "        # Use the QA agent to answer a question\n",
        "        qa_response = qa_agent.answer(query)\n",
        "        print(f\"QA Agent Response for '{query}': {qa_response}\\n\")\n",
        "\n",
        "    # Simulate a code generation query\n",
        "    code_queries = [\n",
        "        \"Write a Python function to add two numbers.\",\n",
        "        \"Generate a Python script to read a CSV file and print its contents.\",\n",
        "    ]\n",
        "\n",
        "    for code_query in code_queries:\n",
        "        # Use the code agent to generate code\n",
        "        code_response = code_agent.generate(code_query)\n",
        "        print(f\"Code Generation Response for '{code_query}': {code_response}\\n\")\n"
      ],
      "metadata": {
        "id": "nYDJdxOy-8-N"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the demo\n",
        "run_demo()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldJOVZpQ--7l",
        "outputId": "af30411a-5a83-452b-af10-429043ea0f18"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING|utils.py:1416] 2024-06-12 15:14:36,224 >> The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "[WARNING|utils.py:1421] 2024-06-12 15:14:36,226 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "[WARNING|utils.py:1416] 2024-06-12 15:14:36,937 >> The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "[WARNING|utils.py:1421] 2024-06-12 15:14:36,939 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Generation Response for 'How do I implement a function in Python?': How do I implement a function in Python?\n",
            "\n",
            "The following code snippet will create a function that\n",
            "\n",
            "QA Agent Response for 'How do I implement a function in Python?': Answering question: How do I implement a function in Python?\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING|utils.py:1416] 2024-06-12 15:14:37,762 >> The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "[WARNING|utils.py:1421] 2024-06-12 15:14:37,763 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Generation Response for 'What is the capital of France?': What is the capital of France?\n",
            "\n",
            "The capital of France is Paris.\n",
            "\n",
            "The capital\n",
            "\n",
            "QA Agent Response for 'What is the capital of France?': Answering question: What is the capital of France?\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING|utils.py:1416] 2024-06-12 15:14:38,504 >> The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "[WARNING|utils.py:1421] 2024-06-12 15:14:38,505 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Generation Response for 'Explain the concept of neural networks.': Explain the concept of neural networks.\n",
            "\n",
            "The first step in understanding neural networks is to understand\n",
            "\n",
            "QA Agent Response for 'Explain the concept of neural networks.': Answering question: Explain the concept of neural networks.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[WARNING|utils.py:1416] 2024-06-12 15:14:39,099 >> The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "[WARNING|utils.py:1421] 2024-06-12 15:14:39,100 >> Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Generation Response for 'What are the benefits of using Python for data science?': What are the benefits of using Python for data science?\n",
            "\n",
            "Python is a powerful programming language that\n",
            "\n",
            "QA Agent Response for 'What are the benefits of using Python for data science?': Answering question: What are the benefits of using Python for data science?\n",
            "\n",
            "Text Generation Response for 'Generate a Python function to calculate the factorial of a number.': Generate a Python function to calculate the factorial of a number.\n",
            "\n",
            ">>> from math import\n",
            "\n",
            "QA Agent Response for 'Generate a Python function to calculate the factorial of a number.': Answering question: Generate a Python function to calculate the factorial of a number.\n",
            "\n",
            "Code Generation Response for 'Write a Python function to add two numbers.': Generating code for: Write a Python function to add two numbers.\n",
            "\n",
            "Code Generation Response for 'Generate a Python script to read a CSV file and print its contents.': Generating code for: Generate a Python script to read a CSV file and print its contents.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ffyDf0FN_AzJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}